{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification German News with BERT\n",
    "\n",
    "In this notebook I follow the example given by Joel in \n",
    "[Bert Fine tuning](https://colab.research.google.com/github/AdvancedNLP/encoder/blob/exercise/BERT_doctors_review.ipynb)\n",
    "\n",
    "The example showed a binary classification problem where as the German Newsset is a multiclass classification.\n",
    "Challenge was to get all the Tensorflow stuff correct.\n",
    "\n",
    "---\n",
    "\n",
    "Fine tuning of a pretrained Hugging Face transfomer\n",
    "In this notebook we will be looking at the fine-tuning process of a BERT model that was previously pre-trained on a large german text corpus. We aim at building a classifier to predict doctor ratings from patients' text comments.\n",
    "\n",
    "A detailed description of the German language reviews of doctors by patients 2019 dataset can be found here\n",
    "\n",
    "For the feature creation and the modeling, we will use the Hugging Face implementation of transformers for Tensorflow 2.0. Transformers provides a general architecture implementation for several state of the art models in the natural language domain.\n",
    "\n",
    "NOTE: This notebook and its implementation is heavily influenced by the data-drive Natural Language Processing of German texts blog post\n",
    "\n",
    "\n",
    "### Results F1 Score\n",
    "As a surprise the results (see last column) are even better as the one achieved with the SimpleTransformer model. Which should actually to same but more sophisticated.\n",
    "\n",
    "![classification_results_with_bert_tf.png](classification_results_with_bert_tf.png)\n",
    "\n",
    "\n",
    "Also check https://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.9.2\n",
      "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 892 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.9.2) (0.8)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.9.2) (21.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 9.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.9.2) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.9.2) (4.6.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.9.2) (1.19.5)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[K     |████████████████████████████████| 748 kB 89.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.2) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.9.2) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.9.2) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests->transformers==4.9.2) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.9.2) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.9.2) (1.26.6)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.2) (3.5.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.9.2) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: filelock, pyyaml, tokenizers, tqdm, huggingface-hub, joblib, click, regex, sacremoses, transformers\n",
      "Successfully installed click-8.0.3 filelock-3.3.2 huggingface-hub-0.0.12 joblib-1.1.0 pyyaml-6.0 regex-2021.11.10 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.9.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers==4.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fhnw-nlp-utils>=0.1.6 in /usr/local/lib/python3.6/dist-packages (0.2.14)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (4.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (5.8.0)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.6.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (0.24.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (0.70.12.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (4.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (3.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (4.62.3)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (2.26.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.6) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.6) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.6) (8.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.6) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.6) (1.5.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.6) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.6/dist-packages (from multiprocess->fhnw-nlp-utils>=0.1.6) (0.3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4->gdown->fhnw-nlp-utils>=0.1.6) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->fhnw-nlp-utils>=0.1.6) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->fhnw-nlp-utils>=0.1.6) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->fhnw-nlp-utils>=0.1.6) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]->gdown->fhnw-nlp-utils>=0.1.6) (2.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->fhnw-nlp-utils>=0.1.6) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from click->nltk->fhnw-nlp-utils>=0.1.6) (4.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.6) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.6) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'fhnw-nlp-utils>=0.1.6'\n",
    "!pip install pyarrow\n",
    "\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from fhnw.nlp.utils.storage import download\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/german_news_articles_original_train_and_test_tokenized.parq\"\n",
    "data_all = load_dataframe(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>Rapid wurde nach dem 0:1 gegen Schachtar im Pl...</td>\n",
       "      <td>Sport</td>\n",
       "      <td>train</td>\n",
       "      <td>Rapid wurde nach dem gegen Schachtar im Playof...</td>\n",
       "      <td>[rapid, wurde, schachtar, playoff, hinspiel, c...</td>\n",
       "      <td>[rapid, schachtar, playoff, hinspiel, champion...</td>\n",
       "      <td>[rapid, wurd, schachtar, playoff, hinspiel, ch...</td>\n",
       "      <td>[rapid, wurde, schachtar, playoff, hinspiel, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>Ökonomen sehen trotz höherer Arbeitslosigkeit ...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>test</td>\n",
       "      <td>Ökonomen sehen trotz höherer Arbeitslosigkeit ...</td>\n",
       "      <td>[ökonomen, sehen, trotz, höherer, arbeitslosig...</td>\n",
       "      <td>[ökonom, sehen, trotzen, hoch, arbeitslosigkei...</td>\n",
       "      <td>[okonom, seh, trotz, hoh, arbeitslos, schuld, ...</td>\n",
       "      <td>[ökonomen, sehen, trotz, höherer, arbeitslosig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7241</th>\n",
       "      <td>Der Oberste Gerichtshof entschied, dass Martin...</td>\n",
       "      <td>Panorama</td>\n",
       "      <td>train</td>\n",
       "      <td>Der Oberste Gerichtshof entschied dass Martin ...</td>\n",
       "      <td>[oberste, gerichtshof, entschied, martin, ball...</td>\n",
       "      <td>[oberst, gerichtshof, entscheiden, martin, bal...</td>\n",
       "      <td>[oberst, gerichtshof, entschied, martin, ballu...</td>\n",
       "      <td>[oberste, gerichtshof, entschied, martin, ball...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_original       label  split  \\\n",
       "4676  Rapid wurde nach dem 0:1 gegen Schachtar im Pl...       Sport  train   \n",
       "612   Ökonomen sehen trotz höherer Arbeitslosigkeit ...  Wirtschaft   test   \n",
       "7241  Der Oberste Gerichtshof entschied, dass Martin...    Panorama  train   \n",
       "\n",
       "                                             text_clean  \\\n",
       "4676  Rapid wurde nach dem gegen Schachtar im Playof...   \n",
       "612   Ökonomen sehen trotz höherer Arbeitslosigkeit ...   \n",
       "7241  Der Oberste Gerichtshof entschied dass Martin ...   \n",
       "\n",
       "                                            token_clean  \\\n",
       "4676  [rapid, wurde, schachtar, playoff, hinspiel, c...   \n",
       "612   [ökonomen, sehen, trotz, höherer, arbeitslosig...   \n",
       "7241  [oberste, gerichtshof, entschied, martin, ball...   \n",
       "\n",
       "                                            token_lemma  \\\n",
       "4676  [rapid, schachtar, playoff, hinspiel, champion...   \n",
       "612   [ökonom, sehen, trotzen, hoch, arbeitslosigkei...   \n",
       "7241  [oberst, gerichtshof, entscheiden, martin, bal...   \n",
       "\n",
       "                                             token_stem  \\\n",
       "4676  [rapid, wurd, schachtar, playoff, hinspiel, ch...   \n",
       "612   [okonom, seh, trotz, hoh, arbeitslos, schuld, ...   \n",
       "7241  [oberst, gerichtshof, entschied, martin, ballu...   \n",
       "\n",
       "                                  token_clean_stopwords  \n",
       "4676  [rapid, wurde, schachtar, playoff, hinspiel, c...  \n",
       "612   [ökonomen, sehen, trotz, höherer, arbeitslosig...  \n",
       "7241  [oberste, gerichtshof, entschied, martin, ball...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be605b8395df4f468610f9b463eb5ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34bc7e4e1cd4a78b6b74c66f1833be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f18b7b4924e4ba8be4b2662689d2aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/485k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008966c9b8df45bbbce2eb78e1455059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to tokenize out news \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 9\n"
     ]
    }
   ],
   "source": [
    "# As well we must label encode the classes -> Be aware that if not using One-Hot Encoding we will need \n",
    "# sparse_categorical_crossentropy as loss as opposed to  categorical_crossentropy if using One-Hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data_all.drop(columns=['label_encoded', 'label_onehot'], errors='ignore', inplace=True)\n",
    "data_all['label_encoded'] = encoder.fit_transform(data_all['label'])\n",
    "\n",
    "num_labels = len(encoder.classes_)\n",
    "print(f'Number of classes {num_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try always to use the same split to be able to compare the results\n",
    "data_train_orig = data_all.loc[(data_all[\"split\"] == \"train\")]\n",
    "data_test_orig = data_all.loc[(data_all[\"split\"] == \"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with all other models we will not use the test data during the training\n",
    "# This makes f1 scores comparable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "    data_train_orig, \n",
    "    data_train_orig[\"label_encoded\"], \n",
    "    random_state=1, \n",
    "    test_size=0.25, \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_orig['token_clean'].apply(len).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 512\n",
    "\n",
    "def tokenize(review):\n",
    "  encoded = tokenizer.encode_plus(\n",
    "      text=review,\n",
    "      add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "      max_length=MAXLEN,  # Max length to truncate/pad\n",
    "      padding='max_length',  # Pad sentence to max length\n",
    "      return_attention_mask=False,  # attention mask not needed for our task\n",
    "      return_token_type_ids=False,\n",
    "      truncation=True, )\n",
    "    \n",
    "  return encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jähriger fällt wohl bis Saisonende aus Wien Rapid muss wohl bis Saisonende auf Offensivspieler Thomas Murg verzichten Der im Winter aus Ried gekommene Jährige erlitt beim Heimdebakel gegen Admira Wacker Mödling am Samstag einen Teilriss des Innenbandes im linken Knie wie eine Magnetresonanz Untersuchung am Donnerstag ergab Murg erhielt eine Schiene muss aber nicht operiert werden Dennoch steht ihm eine mehrwöchige Pause bevor \n",
      "\n",
      "The first 5 entries of the tokenized string [3, 112, 352, 519, 5483]\n"
     ]
    }
   ],
   "source": [
    "text = data_train_orig['text_clean'].iloc[0]\n",
    "print(text)\n",
    "print(f'\\nThe first 5 entries of the tokenized string {tokenize(text)[0:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6933/6933 [00:38<00:00, 178.04it/s]\n",
      "100%|██████████| 2312/2312 [00:13<00:00, 174.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train and test data\n",
    "import tqdm\n",
    "train_input_ids = np.array([tokenize(review) for review in tqdm.tqdm(train_ids['text_clean'])])\n",
    "test_input_ids = np.array([tokenize(review) for review in tqdm.tqdm(test_ids['text_clean'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to reduce BATCH_SIZE to 4 otherwise on my GPU RTX-2060 I was always runnging into OOM\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((train_input_ids, train_labels))\n",
    "                    .shuffle(buffer_size=len(train_input_ids), reshuffle_each_iteration=True)\n",
    "                    .repeat(EPOCHS)\n",
    "                    .batch(BATCH_SIZE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((test_input_ids, test_labels))\n",
    "                    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len=MAXLEN):\n",
    "    \"\"\" add multi class classification to pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
    "    encoder_outputs = bert_model(input_word_ids)\n",
    "\n",
    "    pooler_output = encoder_outputs[1]\n",
    "    cls_embedding = pooler_output\n",
    "    \n",
    "    # Need the number of classed for the Dense layer\n",
    "    no_classes = len(encoder.classes_)\n",
    "    stack = tf.keras.layers.Dense(no_classes)(cls_embedding)\n",
    "    # Multi class os use softmax\n",
    "    output = tf.keras.layers.Activation('softmax')(stack)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b3316f0ee544218bd6c3b153a85821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-german-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fef943dd528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fef9930b1d8> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fef943dd528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fef9930b1d8> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  TFBaseModelOutputWithPool 109081344 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 6921      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 109,088,265\n",
      "Trainable params: 109,088,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_len=MAXLEN)\n",
    "# This did not make the training signifcant faster -> do not know why.\n",
    "# So I allowrd to train all params\n",
    "#model.layers[1].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    # See https://stackoverflow.com/questions/62148508/how-can-i-overcome-valueerror-shapes-none-1-and-none-7-are-incompatible\n",
    "    loss = \"sparse_categorical_crossentropy\" # \"categorical_crossentropy\"\n",
    "    model.compile(optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1733/1733 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9919WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1733/1733 [==============================] - 672s 383ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.5939 - val_accuracy: 0.8888\n",
      "\n",
      "Epoch 00001: saving model to training_berts_final/cp.ckpt\n",
      "Epoch 2/4\n",
      "1733/1733 [==============================] - 668s 385ms/step - loss: 0.0322 - accuracy: 0.9918 - val_loss: 0.5150 - val_accuracy: 0.8940\n",
      "\n",
      "Epoch 00002: saving model to training_berts_final/cp.ckpt\n",
      "Epoch 3/4\n",
      "1733/1733 [==============================] - 669s 386ms/step - loss: 0.0256 - accuracy: 0.9921 - val_loss: 0.6332 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00003: saving model to training_berts_final/cp.ckpt\n",
      "Epoch 4/4\n",
      "1733/1733 [==============================] - 670s 387ms/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.6184 - val_accuracy: 0.8945\n",
      "\n",
      "Epoch 00004: saving model to training_berts_final/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "model = model_loaded\n",
    "checkpoint_path = \"training_berts_final/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=int(np.floor((len(train_input_ids) / BATCH_SIZE))),\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[cp_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Warning\n",
    "Could not use mode.save_model()\n",
    "It was not possible to load the model again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-german-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fee2d411828>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint('training_berts_final')\n",
    "model_loaded = build_model(max_len=MAXLEN)\n",
    "model_loaded.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel TFBaseModelOutputWithPool 109081344 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 6921      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 109,088,265\n",
      "Trainable params: 109,088,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loaded = compile_model(model_loaded)\n",
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 - 58s - loss: 0.6184 - accuracy: 0.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6184117197990417, 0.8944636583328247]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.evaluate(test_input_ids, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [00:06<00:00, 167.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate tokens of test data\n",
    "import tqdm\n",
    "\n",
    "data_test_orig_ids = np.array([tokenize(review) for review in tqdm.tqdm(data_test_orig['text_clean'])])\n",
    "data_test_orig_labels = data_test_orig[\"label_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 26s 790ms/step - loss: 0.4593 - accuracy: 0.8901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4593358337879181, 0.8900778293609619]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.evaluate(data_test_orig_ids, data_test_orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "257/257 - 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.6393076e-04, 7.4043286e-01, 1.2882872e-04, 9.5944946e-05,\n",
       "       3.2380727e-04, 6.8556066e-05, 1.8239823e-04, 2.5813976e-01,\n",
       "       3.6388525e-04], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the predictions for the test data\n",
    "# Will return of each sample an array of len 9 with the probabilities for every class\n",
    "predictions = model_loaded.predict(data_test_orig_ids, batch_size=BATCH_SIZE, verbose=2, use_multiprocessing=True)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91        67\n",
      "           1       0.79      0.78      0.79       102\n",
      "           2       0.91      0.88      0.90       151\n",
      "           3       0.89      0.89      0.89        54\n",
      "           4       0.78      0.85      0.81       168\n",
      "           5       0.98      0.99      0.99       120\n",
      "           6       0.98      0.96      0.97       168\n",
      "           7       0.90      0.82      0.86       141\n",
      "           8       0.79      0.91      0.85        57\n",
      "\n",
      "    accuracy                           0.89      1028\n",
      "   macro avg       0.88      0.89      0.88      1028\n",
      "weighted avg       0.89      0.89      0.89      1028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the class with the highest probability\n",
    "most_probable_classes = np.argmax(predictions, axis = 1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(data_test_orig_labels, most_probable_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the inverse transform to get from the integer values to the string classes again\n",
    "# Otherwise we will not be able to compare the f1 score with the other models\n",
    "y_test_pred_inv = encoder.inverse_transform(most_probable_classes)\n",
    "y_test_inv = encoder.inverse_transform(data_test_orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All F1 Score - Maximum highlighted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_45b67c22_4569_11ec_a21e_0242ac110002row0_col10,#T_45b67c22_4569_11ec_a21e_0242ac110002row1_col7,#T_45b67c22_4569_11ec_a21e_0242ac110002row2_col10,#T_45b67c22_4569_11ec_a21e_0242ac110002row3_col2,#T_45b67c22_4569_11ec_a21e_0242ac110002row4_col6,#T_45b67c22_4569_11ec_a21e_0242ac110002row5_col0,#T_45b67c22_4569_11ec_a21e_0242ac110002row5_col2,#T_45b67c22_4569_11ec_a21e_0242ac110002row5_col7,#T_45b67c22_4569_11ec_a21e_0242ac110002row6_col10,#T_45b67c22_4569_11ec_a21e_0242ac110002row7_col9,#T_45b67c22_4569_11ec_a21e_0242ac110002row8_col0,#T_45b67c22_4569_11ec_a21e_0242ac110002row9_col9,#T_45b67c22_4569_11ec_a21e_0242ac110002row9_col10,#T_45b67c22_4569_11ec_a21e_0242ac110002row10_col9,#T_45b67c22_4569_11ec_a21e_0242ac110002row11_col10{\n",
       "            background-color:  lightblue;\n",
       "        }</style><table id=\"T_45b67c22_4569_11ec_a21e_0242ac110002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1-baseline</th>        <th class=\"col_heading level0 col1\" >f1-one vs one</th>        <th class=\"col_heading level0 col2\" >f1-baseline 2 Gram</th>        <th class=\"col_heading level0 col3\" >f1-kNN</th>        <th class=\"col_heading level0 col4\" >f1-Random Forest</th>        <th class=\"col_heading level0 col5\" >f1-Naiv Bayes</th>        <th class=\"col_heading level0 col6\" >f1-baseline stemming</th>        <th class=\"col_heading level0 col7\" >f1-baseline stemming optimized</th>        <th class=\"col_heading level0 col8\" >f1-dummy clf</th>        <th class=\"col_heading level0 col9\" >f1-simpletransformer</th>        <th class=\"col_heading level0 col10\" >f1-bert-tf-transformer</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row0\" class=\"row_heading level0 row0\" >Etat</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col0\" class=\"data row0 col0\" >0.850394</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col1\" class=\"data row0 col1\" >0.848000</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col2\" class=\"data row0 col2\" >0.806723</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col3\" class=\"data row0 col3\" >0.753846</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col4\" class=\"data row0 col4\" >0.568421</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col5\" class=\"data row0 col5\" >0.029412</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col6\" class=\"data row0 col6\" >0.852713</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col7\" class=\"data row0 col7\" >0.859375</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col8\" class=\"data row0 col8\" >0.031746</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col9\" class=\"data row0 col9\" >0.884058</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row0_col10\" class=\"data row0 col10\" >0.909091</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row1\" class=\"row_heading level0 row1\" >Inland</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col0\" class=\"data row1 col0\" >0.835821</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col1\" class=\"data row1 col1\" >0.827586</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col2\" class=\"data row1 col2\" >0.831683</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col3\" class=\"data row1 col3\" >0.770563</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col4\" class=\"data row1 col4\" >0.780488</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col5\" class=\"data row1 col5\" >0.717647</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col6\" class=\"data row1 col6\" >0.848485</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col7\" class=\"data row1 col7\" >0.869565</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col8\" class=\"data row1 col8\" >0.040201</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col9\" class=\"data row1 col9\" >0.857143</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row1_col10\" class=\"data row1 col10\" >0.788177</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row2\" class=\"row_heading level0 row2\" >International</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col0\" class=\"data row2 col0\" >0.855172</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col1\" class=\"data row2 col1\" >0.849123</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col2\" class=\"data row2 col2\" >0.823529</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col3\" class=\"data row2 col3\" >0.813115</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col4\" class=\"data row2 col4\" >0.788274</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col5\" class=\"data row2 col5\" >0.830450</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col6\" class=\"data row2 col6\" >0.851351</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col7\" class=\"data row2 col7\" >0.851351</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col8\" class=\"data row2 col8\" >0.190184</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col9\" class=\"data row2 col9\" >0.876254</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row2_col10\" class=\"data row2 col10\" >0.895623</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row3\" class=\"row_heading level0 row3\" >Kultur</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col0\" class=\"data row3 col0\" >0.854545</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col1\" class=\"data row3 col1\" >0.811321</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col2\" class=\"data row3 col2\" >0.890909</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col3\" class=\"data row3 col3\" >0.742268</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col4\" class=\"data row3 col4\" >0.804124</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col6\" class=\"data row3 col6\" >0.859813</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col7\" class=\"data row3 col7\" >0.878505</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col8\" class=\"data row3 col8\" >0.055556</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col9\" class=\"data row3 col9\" >0.854545</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row3_col10\" class=\"data row3 col10\" >0.888889</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row4\" class=\"row_heading level0 row4\" >Panorama</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col0\" class=\"data row4 col0\" >0.829412</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col1\" class=\"data row4 col1\" >0.802228</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col2\" class=\"data row4 col2\" >0.804598</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col3\" class=\"data row4 col3\" >0.738028</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col4\" class=\"data row4 col4\" >0.717087</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col5\" class=\"data row4 col5\" >0.631791</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col6\" class=\"data row4 col6\" >0.838150</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col7\" class=\"data row4 col7\" >0.826347</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col8\" class=\"data row4 col8\" >0.197015</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col9\" class=\"data row4 col9\" >0.832335</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row4_col10\" class=\"data row4 col10\" >0.809117</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row5\" class=\"row_heading level0 row5\" >Sport</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col0\" class=\"data row5 col0\" >0.991667</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col1\" class=\"data row5 col1\" >0.978903</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col2\" class=\"data row5 col2\" >0.991667</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col3\" class=\"data row5 col3\" >0.962343</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col4\" class=\"data row5 col4\" >0.970464</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col5\" class=\"data row5 col5\" >0.948718</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col6\" class=\"data row5 col6\" >0.987448</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col7\" class=\"data row5 col7\" >0.991667</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col8\" class=\"data row5 col8\" >0.150794</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col9\" class=\"data row5 col9\" >0.978903</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row5_col10\" class=\"data row5 col10\" >0.987552</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row6\" class=\"row_heading level0 row6\" >Web</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col0\" class=\"data row6 col0\" >0.908012</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col1\" class=\"data row6 col1\" >0.900901</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col2\" class=\"data row6 col2\" >0.908012</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col3\" class=\"data row6 col3\" >0.835913</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col4\" class=\"data row6 col4\" >0.850000</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col5\" class=\"data row6 col5\" >0.756219</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col6\" class=\"data row6 col6\" >0.925373</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col7\" class=\"data row6 col7\" >0.923077</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col8\" class=\"data row6 col8\" >0.127796</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col9\" class=\"data row6 col9\" >0.917933</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row6_col10\" class=\"data row6 col10\" >0.969880</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row7\" class=\"row_heading level0 row7\" >Wirtschaft</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col0\" class=\"data row7 col0\" >0.849315</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col1\" class=\"data row7 col1\" >0.813559</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col2\" class=\"data row7 col2\" >0.823129</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col3\" class=\"data row7 col3\" >0.747253</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col4\" class=\"data row7 col4\" >0.719723</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col5\" class=\"data row7 col5\" >0.785965</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col6\" class=\"data row7 col6\" >0.851211</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col7\" class=\"data row7 col7\" >0.844291</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col8\" class=\"data row7 col8\" >0.141343</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col9\" class=\"data row7 col9\" >0.877193</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row7_col10\" class=\"data row7 col10\" >0.855019</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row8\" class=\"row_heading level0 row8\" >Wissenschaft</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col0\" class=\"data row8 col0\" >0.924370</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col1\" class=\"data row8 col1\" >0.902655</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col2\" class=\"data row8 col2\" >0.905983</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col3\" class=\"data row8 col3\" >0.796117</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col4\" class=\"data row8 col4\" >0.844037</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col6\" class=\"data row8 col6\" >0.905983</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col7\" class=\"data row8 col7\" >0.905983</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col8\" class=\"data row8 col8\" >0.052632</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col9\" class=\"data row8 col9\" >0.877193</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row8_col10\" class=\"data row8 col10\" >0.845528</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row9\" class=\"row_heading level0 row9\" >accuracy</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col0\" class=\"data row9 col0\" >0.876459</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col1\" class=\"data row9 col1\" >0.857977</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col2\" class=\"data row9 col2\" >0.861868</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col3\" class=\"data row9 col3\" >0.799611</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col4\" class=\"data row9 col4\" >0.790856</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col5\" class=\"data row9 col5\" >0.694553</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col6\" class=\"data row9 col6\" >0.880350</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col7\" class=\"data row9 col7\" >0.881323</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col8\" class=\"data row9 col8\" >0.131323</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col9\" class=\"data row9 col9\" >0.885214</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row9_col10\" class=\"data row9 col10\" >0.885214</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row10\" class=\"row_heading level0 row10\" >macro avg</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col0\" class=\"data row10 col0\" >0.877634</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col1\" class=\"data row10 col1\" >0.859364</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col2\" class=\"data row10 col2\" >0.865137</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col3\" class=\"data row10 col3\" >0.795494</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col4\" class=\"data row10 col4\" >0.782513</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col5\" class=\"data row10 col5\" >0.522245</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col6\" class=\"data row10 col6\" >0.880059</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col7\" class=\"data row10 col7\" >0.883351</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col8\" class=\"data row10 col8\" >0.109696</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col9\" class=\"data row10 col9\" >0.883951</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row10_col10\" class=\"data row10 col10\" >0.883208</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_45b67c22_4569_11ec_a21e_0242ac110002level0_row11\" class=\"row_heading level0 row11\" >weighted avg</th>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col0\" class=\"data row11 col0\" >0.876300</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col1\" class=\"data row11 col1\" >0.858965</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col2\" class=\"data row11 col2\" >0.861639</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col3\" class=\"data row11 col3\" >0.800207</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col4\" class=\"data row11 col4\" >0.787416</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col5\" class=\"data row11 col5\" >0.640488</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col6\" class=\"data row11 col6\" >0.880436</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col7\" class=\"data row11 col7\" >0.881183</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col8\" class=\"data row11 col8\" >0.129901</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col9\" class=\"data row11 col9\" >0.885523</td>\n",
       "                        <td id=\"T_45b67c22_4569_11ec_a21e_0242ac110002row11_col10\" class=\"data row11 col10\" >0.885868</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fef9d24d9b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the f1 score to the file with the scores of the baseline models and the experiments\n",
    "import os\n",
    "import class_result\n",
    "\n",
    "filename = 'data/overview_classification_results-with-transformer.csv'\n",
    "df_classification_results = class_result.append_to_classification_report('f1-bert-tf-transformer', y_test_inv, y_test_pred_inv, filename=filename)\n",
    "\n",
    "print(\"All F1 Score - Maximum highlighted\")\n",
    "df_classification_results.style.highlight_max(color = 'lightblue', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_c19c732c_4b50_11ec_a222_0242ac110002row0_col2,#T_c19c732c_4b50_11ec_a222_0242ac110002row1_col1,#T_c19c732c_4b50_11ec_a222_0242ac110002row2_col2,#T_c19c732c_4b50_11ec_a222_0242ac110002row3_col1,#T_c19c732c_4b50_11ec_a222_0242ac110002row4_col2,#T_c19c732c_4b50_11ec_a222_0242ac110002row5_col3,#T_c19c732c_4b50_11ec_a222_0242ac110002row6_col3,#T_c19c732c_4b50_11ec_a222_0242ac110002row7_col3,#T_c19c732c_4b50_11ec_a222_0242ac110002row8_col0,#T_c19c732c_4b50_11ec_a222_0242ac110002row9_col3,#T_c19c732c_4b50_11ec_a222_0242ac110002row10_col2,#T_c19c732c_4b50_11ec_a222_0242ac110002row11_col3{\n",
       "            background-color:  lightblue;\n",
       "        }</style><table id=\"T_c19c732c_4b50_11ec_a222_0242ac110002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1-baseline</th>        <th class=\"col_heading level0 col1\" >f1-baseline stemming optimized</th>        <th class=\"col_heading level0 col2\" >f1-simpletransformer</th>        <th class=\"col_heading level0 col3\" >f1-bert-tf-transformer</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row0\" class=\"row_heading level0 row0\" >Etat</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row0_col0\" class=\"data row0 col0\" >0.850394</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row0_col1\" class=\"data row0 col1\" >0.859375</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row0_col2\" class=\"data row0 col2\" >0.884058</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row0_col3\" class=\"data row0 col3\" >0.868217</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row1\" class=\"row_heading level0 row1\" >Inland</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row1_col0\" class=\"data row1 col0\" >0.835821</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row1_col1\" class=\"data row1 col1\" >0.869565</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row1_col2\" class=\"data row1 col2\" >0.857143</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row1_col3\" class=\"data row1 col3\" >0.840000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row2\" class=\"row_heading level0 row2\" >International</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row2_col0\" class=\"data row2 col0\" >0.855172</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row2_col1\" class=\"data row2 col1\" >0.851351</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row2_col2\" class=\"data row2 col2\" >0.876254</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row2_col3\" class=\"data row2 col3\" >0.874598</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row3\" class=\"row_heading level0 row3\" >Kultur</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row3_col0\" class=\"data row3 col0\" >0.854545</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row3_col1\" class=\"data row3 col1\" >0.878505</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row3_col2\" class=\"data row3 col2\" >0.854545</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row3_col3\" class=\"data row3 col3\" >0.838095</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row4\" class=\"row_heading level0 row4\" >Panorama</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row4_col0\" class=\"data row4 col0\" >0.829412</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row4_col1\" class=\"data row4 col1\" >0.826347</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row4_col2\" class=\"data row4 col2\" >0.832335</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row4_col3\" class=\"data row4 col3\" >0.821317</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row5\" class=\"row_heading level0 row5\" >Sport</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row5_col0\" class=\"data row5 col0\" >0.991667</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row5_col1\" class=\"data row5 col1\" >0.991667</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row5_col2\" class=\"data row5 col2\" >0.978903</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row5_col3\" class=\"data row5 col3\" >0.991736</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row6\" class=\"row_heading level0 row6\" >Web</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row6_col0\" class=\"data row6 col0\" >0.908012</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row6_col1\" class=\"data row6 col1\" >0.923077</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row6_col2\" class=\"data row6 col2\" >0.917933</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row6_col3\" class=\"data row6 col3\" >0.947059</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row7\" class=\"row_heading level0 row7\" >Wirtschaft</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row7_col0\" class=\"data row7 col0\" >0.849315</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row7_col1\" class=\"data row7 col1\" >0.844291</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row7_col2\" class=\"data row7 col2\" >0.877193</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row7_col3\" class=\"data row7 col3\" >0.923077</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row8\" class=\"row_heading level0 row8\" >Wissenschaft</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row8_col0\" class=\"data row8 col0\" >0.924370</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row8_col1\" class=\"data row8 col1\" >0.905983</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row8_col2\" class=\"data row8 col2\" >0.877193</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row8_col3\" class=\"data row8 col3\" >0.822581</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row9\" class=\"row_heading level0 row9\" >accuracy</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row9_col0\" class=\"data row9 col0\" >0.876459</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row9_col1\" class=\"data row9 col1\" >0.881323</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row9_col2\" class=\"data row9 col2\" >0.885214</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row9_col3\" class=\"data row9 col3\" >0.890078</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row10\" class=\"row_heading level0 row10\" >macro avg</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row10_col0\" class=\"data row10 col0\" >0.877634</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row10_col1\" class=\"data row10 col1\" >0.883351</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row10_col2\" class=\"data row10 col2\" >0.883951</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row10_col3\" class=\"data row10 col3\" >0.880742</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c19c732c_4b50_11ec_a222_0242ac110002level0_row11\" class=\"row_heading level0 row11\" >weighted avg</th>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row11_col0\" class=\"data row11 col0\" >0.876300</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row11_col1\" class=\"data row11 col1\" >0.881183</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row11_col2\" class=\"data row11 col2\" >0.885523</td>\n",
       "                        <td id=\"T_c19c732c_4b50_11ec_a222_0242ac110002row11_col3\" class=\"data row11 col3\" >0.889405</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fcb9c05ca58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop not needed columns (just for the summary notebook)\n",
    "filename = 'data/overview_classification_results-with-transformer.csv'\n",
    "df_classification_results = pd.read_csv(filename, index_col=0)\n",
    "df_classification_results[['f1-baseline', 'f1-baseline stemming optimized', 'f1-simpletransformer', 'f1-bert-tf-transformer']].style.highlight_max(color = 'lightblue', axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
